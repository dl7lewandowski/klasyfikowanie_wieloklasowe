{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5da88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1608d349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rps.zip', <http.client.HTTPMessage at 0x7f177d1b1880>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip'\n",
    "file_name = 'rps.zip'\n",
    "training_dir = './tmp/rps/'\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8e30923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('rps-test-set.zip', <http.client.HTTPMessage at 0x7f17586ab250>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url ='https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip'\n",
    "file_name = 'rps-test-set.zip'\n",
    "validation_dir = './tmp/rps-test-set/'\n",
    "urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e92f2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall('./tmp/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b78fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_ref = zipfile.ZipFile(file_name, 'r')\n",
    "zip_ref.extractall('./tmp/')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e687f8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "validation_generator = validation_datagen.flow_from_directory(validation_dir, target_size=(150, 150), class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b84fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2520 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_datagen = ImageDataGenerator(rescale=1./255., rotation_range=40, width_shift_range=0.2,\n",
    "                                      height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True,\n",
    "                                     fill_mode='nearest')\n",
    "training_generator = training_datagen.flow_from_directory(training_dir, target_size=(150, 150), class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc97c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # pierwsza konwulucja\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # druga konwulucja\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(2,2), \n",
    "    # trzecia konwolucja\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # czwarta konwolucja\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # splaszczenie wynikow, w celu przekazania do warstwy gestej\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # warstwa gesta 512 nerownow\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7e75e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f73dc395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/79 [==============================] - 31s 384ms/step - loss: 2.1362 - accuracy: 0.3637 - val_loss: 1.0262 - val_accuracy: 0.5860\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 29s 369ms/step - loss: 0.9099 - accuracy: 0.5823 - val_loss: 0.2841 - val_accuracy: 0.9758\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 30s 379ms/step - loss: 0.5293 - accuracy: 0.7798 - val_loss: 0.0858 - val_accuracy: 0.9973\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 30s 377ms/step - loss: 0.3406 - accuracy: 0.8801 - val_loss: 0.1484 - val_accuracy: 0.9597\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 30s 382ms/step - loss: 0.2483 - accuracy: 0.9140 - val_loss: 0.1521 - val_accuracy: 0.9462\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 31s 392ms/step - loss: 0.1845 - accuracy: 0.9401 - val_loss: 0.0437 - val_accuracy: 0.9892\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 31s 384ms/step - loss: 0.1323 - accuracy: 0.9497 - val_loss: 1.2700 - val_accuracy: 0.6478\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 34s 428ms/step - loss: 0.2057 - accuracy: 0.9344 - val_loss: 0.1091 - val_accuracy: 0.9785\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 35s 445ms/step - loss: 0.1231 - accuracy: 0.9620 - val_loss: 0.1900 - val_accuracy: 0.9435\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 35s 438ms/step - loss: 0.0827 - accuracy: 0.9736 - val_loss: 0.0964 - val_accuracy: 0.9651\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_generator, epochs=10, validation_data=validation_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9066549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './tmp/test-data/'\n",
    "zip_ref = zipfile.ZipFile('./rps-validation.zip', 'r')\n",
    "zip_ref.extractall(file_name)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d04f9eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper8.png\n",
      "[1. 0. 0.]\n",
      "rock7.png\n",
      "[0. 1. 0.]\n",
      "scissors6.png\n",
      "[0. 0. 1.]\n",
      "paper9.png\n",
      "[1. 0. 0.]\n",
      "scissors-hires1.png\n",
      "[0. 0. 1.]\n",
      "paper-hires1.png\n",
      "[1. 0. 0.]\n",
      "rock3.png\n",
      "[0. 1. 0.]\n",
      "rock-hires1.png\n",
      "[0. 1. 0.]\n",
      "paper4.png\n",
      "[1. 0. 0.]\n",
      "paper-hires2.png\n",
      "[1. 0. 0.]\n",
      "scissors3.png\n",
      "[0. 0. 1.]\n",
      "scissors5.png\n",
      "[0. 0. 1.]\n",
      "scissors8.png\n",
      "[0. 0. 1.]\n",
      "rock1.png\n",
      "[0. 1. 0.]\n",
      "paper5.png\n",
      "[1. 0. 0.]\n",
      "paper1.png\n",
      "[1. 0. 0.]\n",
      "paper6.png\n",
      "[1. 0. 0.]\n",
      "rock-hires2.png\n",
      "[0. 1. 0.]\n",
      "scissors-hires2.png\n",
      "[0. 0. 1.]\n",
      "scissors1.png\n",
      "[0. 0. 1.]\n",
      "rock9.png\n",
      "[0. 1. 0.]\n",
      "rock8.png\n",
      "[0. 1. 0.]\n",
      "scissors7.png\n",
      "[0. 0. 1.]\n",
      "rock6.png\n",
      "[0. 1. 0.]\n",
      "rock4.png\n",
      "[0. 1. 0.]\n",
      "paper2.png\n",
      "[1. 0. 0.]\n",
      "scissors9.png\n",
      "[0. 0. 1.]\n",
      "rock5.png\n",
      "[0. 1. 0.]\n",
      "scissors4.png\n",
      "[0. 0. 1.]\n",
      "paper7.png\n",
      "[1. 0. 0.]\n",
      "scissors2.png\n",
      "[0. 0. 1.]\n",
      "rock2.png\n",
      "[0. 1. 0.]\n",
      "paper3.png\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# testujemy model na danych ktorych nie widzial\n",
    "\n",
    "directory =  './tmp/test-data/'\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith('png'):\n",
    "        path = './tmp/test-data/' + file\n",
    "        \n",
    "        img = image.load_img(path, target_size=(150, 150))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        \n",
    "        images = np.vstack([x])\n",
    "        classes = model.predict(images, batch_size=10)\n",
    "        print(file)\n",
    "        print(classes[0])\n",
    "        \n",
    "\n",
    "# Wyniki: klasy sa wczytane w kolejnosci alfabetycznej(ang) (paper, rock, scissors) co odpoiwada [1, 0, 0 ] - paper,\n",
    "# a w przypadku - [0, 1, 0] - rock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8233620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[0][1] == np.float32(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f8378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
